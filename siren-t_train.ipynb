{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gplib import GenPhotonLib\n",
    "from slar.io import PhotonLibDataset, PLibDataLoader\n",
    "from sirent_vis import SirenTVis\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "cfg = yaml.safe_load(\"\"\"\n",
    "photonlib:\n",
    "        filepath: data/ptlib_2x2_module0.h5\n",
    "                \n",
    "model:\n",
    "    network:\n",
    "        in_features: 3\n",
    "        hidden_features: [512, 256, 256]\n",
    "        hidden_layers: [5, 3, 3]\n",
    "        out_features: [48, 4800]\n",
    "    ckpt_file: \"\"\n",
    "    output_scale:\n",
    "        fix: True\n",
    "transform_vis:\n",
    "    vmax: 1.0\n",
    "    eps: 1.e-7\n",
    "    sin_out: True\n",
    "data:\n",
    "    dataset:\n",
    "        device: 'cuda:0'\n",
    "        weight:\n",
    "            method: \"vis\"\n",
    "            threshold: 1.e-8\n",
    "            factor: 1.e+6\n",
    "    loader:\n",
    "            batch_size: 16384\n",
    "            num_workers: 4\n",
    "            pin_memory: True\n",
    "            drop_last: True\n",
    "            shuffle: true\n",
    "logger:\n",
    "    dir_name: logs\n",
    "    file_name: log.csv\n",
    "    log_every_nstep: 17\n",
    "    analysis:\n",
    "        vis_bias:\n",
    "            threshold: 4.5e-5\n",
    "train:\n",
    "    max_epochs: 2000\n",
    "    save_every_epochs: 10\n",
    "    optimizer_class: Adam\n",
    "    optimizer_param:\n",
    "        lr: 2.e-6\n",
    "    resume: False\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slar.io import PLibDataLoader\n",
    "from slar.nets import WeightedL2Loss\n",
    "from slar.optimizers import optimizer_factory\n",
    "from slar.utils import CSVLogger, get_device\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(cfg: dict):\n",
    "    \"\"\"\n",
    "    A function to run an optimization loop for SirenVis model.\n",
    "    Configuration specific to this function is \"train\" at the top level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_epochs : int\n",
    "        The maximum number of epochs before stopping training\n",
    "\n",
    "    max_iterations : int\n",
    "        The maximum number of iterations before stopping training\n",
    "\n",
    "    save_every_epochs : int\n",
    "        A period in epochs to store the network state\n",
    "\n",
    "    save_every_iterations : int\n",
    "        A period in iterations to store the network state\n",
    "\n",
    "    optimizer_class : str\n",
    "        An optimizer class name to train SirenVis\n",
    "\n",
    "    optimizer_param : dict\n",
    "        Optimizer constructor arguments\n",
    "\n",
    "    resume : bool\n",
    "        If True, and if a checkopint file is provided for the model, resume training\n",
    "        with the optimizer state restored from the last checkpoint step.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if cfg.get(\"device\"):\n",
    "        DEVICE = get_device(cfg[\"device\"][\"type\"])\n",
    "\n",
    "    iteration_ctr = 0\n",
    "    epoch_ctr = 0\n",
    "\n",
    "    # Create necessary pieces: the model, optimizer, loss, logger.\n",
    "    # Load the states if this is resuming.\n",
    "    net = SirenTVis(cfg).to(DEVICE)\n",
    "\n",
    "    dl = PLibDataLoader(cfg, device=DEVICE)\n",
    "\n",
    "    opt, sch, epoch = optimizer_factory(net.parameters(), cfg)\n",
    "    if epoch > 0:\n",
    "        iteration_ctr = int(epoch * len(dl))\n",
    "        epoch_ctr = int(epoch)\n",
    "        print(\n",
    "            \"[train] resuming training from iteration\",\n",
    "            iteration_ctr,\n",
    "            \"epoch\",\n",
    "            epoch_ctr,\n",
    "        )\n",
    "    criterion = WeightedL2Loss()\n",
    "    logger = CSVLogger(cfg)\n",
    "    logdir = logger.logdir\n",
    "\n",
    "    # Set the control parameters for the training loop\n",
    "    train_cfg = cfg.get(\"train\", dict())\n",
    "    epoch_max = train_cfg.get(\"max_epochs\", int(1e20))\n",
    "    iteration_max = train_cfg.get(\"max_iterations\", int(1e20))\n",
    "    save_every_iterations = train_cfg.get(\"save_every_iterations\", -1)\n",
    "    save_every_epochs = train_cfg.get(\"save_every_epochs\", -1)\n",
    "    print(f\"[train] train for max iterations {iteration_max} or max epochs {epoch_max}\")\n",
    "\n",
    "    # Store configuration\n",
    "    with open(os.path.join(logdir, \"train_cfg.yaml\"), \"w\") as f:\n",
    "        yaml.safe_dump(cfg, f)\n",
    "\n",
    "    # Start the training loop\n",
    "    t0 = time.time()\n",
    "    twait = time.time()\n",
    "    stop_training = False\n",
    "\n",
    "    while iteration_ctr < iteration_max and epoch_ctr < epoch_max:\n",
    "        for batch_idx, data in enumerate(tqdm(dl, desc=\"Epoch %-3d\" % epoch_ctr)):\n",
    "            iteration_ctr += 1\n",
    "\n",
    "            # Input data prep\n",
    "            x = data[\"position\"].to(DEVICE)\n",
    "            weights = data[\"weight\"].to(DEVICE)\n",
    "            target = data[\"target\"].to(DEVICE)\n",
    "            target_linear = data[\"value\"].to(DEVICE)\n",
    "\n",
    "            twait = time.time() - twait\n",
    "            # Running the model, compute the loss, back-prop gradients to optimize.\n",
    "            ttrain = time.time()\n",
    "            pred = net(x)\n",
    "            loss = criterion(pred, target, weights)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            ttrain = time.time() - ttrain\n",
    "\n",
    "            # Log training parameters\n",
    "            logger.record(\n",
    "                [\"iter\", \"epoch\", \"loss\", \"ttrain\", \"twait\"],\n",
    "                [iteration_ctr, epoch_ctr, loss.item(), ttrain, twait],\n",
    "            )\n",
    "            twait = time.time()\n",
    "\n",
    "            # Step the logger\n",
    "            pred_linear = dl.inv_xform_vis(pred)\n",
    "            logger.step(iteration_ctr, target_linear, pred_linear)\n",
    "\n",
    "            # Save the model parameters if the condition is met\n",
    "            if save_every_iterations > 0 and iteration_ctr % save_every_iterations == 0:\n",
    "                filename = os.path.join(\n",
    "                    logdir,\n",
    "                    \"iteration-%06d-epoch-%04d.ckpt\" % (iteration_ctr, epoch_ctr),\n",
    "                )\n",
    "                net.save_state(filename, opt, sch, iteration_ctr)\n",
    "\n",
    "            if iteration_max <= iteration_ctr:\n",
    "                stop_training = True\n",
    "                break\n",
    "\n",
    "        if stop_training:\n",
    "            break\n",
    "\n",
    "        if sch is not None:\n",
    "            sch.step()\n",
    "\n",
    "        epoch_ctr += 1\n",
    "\n",
    "        if (save_every_epochs * epoch_ctr) > 0 and epoch_ctr % save_every_epochs == 0:\n",
    "            filename = os.path.join(\n",
    "                logdir, \"iteration-%06d-epoch-%04d.ckpt\" % (iteration_ctr, epoch_ctr)\n",
    "            )\n",
    "            net.save_state(filename, opt, sch, iteration_ctr / len(dl))\n",
    "\n",
    "    print(\"[train] Stopped training at iteration\", iteration_ctr, \"epochs\", epoch_ctr)\n",
    "    logger.write()\n",
    "    logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Siren] 3 in => [48, 4800] out, hidden [512, 256, 256] features [5, 3, 3] layers\n",
      "        omega 30.0 first 30.0 hidden, the final layer linear False\n",
      "[PhotonLib] loading data/ptlib_2x2_module0.h5\n",
      "[PhotonLib] file loaded\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 11.84 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 51\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Create necessary pieces: the model, optimizer, loss, logger.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Load the states if this is resuming.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m net \u001b[38;5;241m=\u001b[39m SirenTVis(cfg)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 51\u001b[0m dl \u001b[38;5;241m=\u001b[39m \u001b[43mPLibDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m opt, sch, epoch \u001b[38;5;241m=\u001b[39m optimizer_factory(net\u001b[38;5;241m.\u001b[39mparameters(), cfg)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/sw/siren-lartpc/slar/io.py:153\u001b[0m, in \u001b[0;36mPLibDataLoader.__init__\u001b[0;34m(self, cfg, device)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03mConstructor.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03mtraining. The final output is scaled to `[0,1]`.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# load plib to device\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plib \u001b[38;5;241m=\u001b[39m \u001b[43mPhotonLib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# get weighting scheme\u001b[39;00m\n\u001b[1;32m    156\u001b[0m weight_cfg \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m,{})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m,{})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/sw/photonlib/photonlib/photonlib.py:92\u001b[0m, in \u001b[0;36mPhotonLib.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PhotonLib(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meff\u001b[38;5;241m.\u001b[39mto(device))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 11.84 GiB. GPU "
     ]
    }
   ],
   "source": [
    "train(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
